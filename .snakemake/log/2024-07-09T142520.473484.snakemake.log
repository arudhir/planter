Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job stats:
job               count
--------------  -------
all                   1
download_reads        1
fastp_raw             1
fastp_trimmed         1
transdecoder          1
trimmomatic           1
trinity               1
total                 7

Select jobs to execute...
Execute 1 jobs...

[Tue Jul  9 14:25:20 2024]
localrule download_reads:
    output: outputs/illumina/raw/SRR2103848_1.fastq.gz, outputs/illumina/raw/SRR2103848_2.fastq.gz
    jobid: 1
    reason: Missing output files: outputs/illumina/raw/SRR2103848_2.fastq.gz, outputs/illumina/raw/SRR2103848_1.fastq.gz
    wildcards: sample=SRR2103848
    resources: tmpdir=/tmp

Terminating processes on user request, this might take some time.
[Tue Jul  9 14:25:28 2024]
Error in rule download_reads:
    jobid: 1
    output: outputs/illumina/raw/SRR2103848_1.fastq.gz, outputs/illumina/raw/SRR2103848_2.fastq.gz
    shell:
        
        fastq-dump --split-files SRR2103848 -O data/
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Complete log: .snakemake/log/2024-07-09T142520.473484.snakemake.log
WorkflowError:
At least one job did not complete successfully.
