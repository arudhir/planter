Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job stats:
job               count
--------------  -------
all                   1
download_reads        1
fastp_raw             1
fastp_trimmed         1
transdecoder          1
trimmomatic           1
trinity               1
total                 7

Select jobs to execute...
Execute 1 jobs...

[Tue Jul  9 14:33:25 2024]
localrule download_reads:
    output: outputs/illumina/raw/SRR2103848_1.fastq.gz, outputs/illumina/raw/SRR2103848_2.fastq.gz
    jobid: 1
    reason: Missing output files: outputs/illumina/raw/SRR2103848_2.fastq.gz, outputs/illumina/raw/SRR2103848_1.fastq.gz
    wildcards: sample=SRR2103848
    resources: tmpdir=/tmp


        fastqer-dump --split-files SRR2103848 -O outputs/illumina/raw
        
[Tue Jul  9 14:33:25 2024]
Error in rule download_reads:
    jobid: 1
    output: outputs/illumina/raw/SRR2103848_1.fastq.gz, outputs/illumina/raw/SRR2103848_2.fastq.gz
    shell:
        
        fastqer-dump --split-files SRR2103848 -O outputs/illumina/raw
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-07-09T143325.146963.snakemake.log
WorkflowError:
At least one job did not complete successfully.
