{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from planter.database.utils.duckdb_utils import (\n",
    "    extract_representative_sequences, \n",
    "    create_duckdb,\n",
    "    merge_duckdbs,\n",
    "    validate_duckdb_schema,\n",
    "    update_clusters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating database: /mnt/data4/recombia.planter/master.duckdb\n",
      "\n",
      "=== Tables in database ===\n",
      "Table: annotations - 2049722 rows\n",
      "Table: clusters - 428497 rows\n",
      "Table: cluster_members - 433791 rows\n",
      "Table: ec_numbers - 571387 rows\n",
      "Table: expression - 3091553 rows\n",
      "Table: gene_protein_map - 745417 rows\n",
      "Table: go_terms - 74845016 rows\n",
      "Table: kegg_info - 1626 rows\n",
      "Table: schema_version - 0 rows\n",
      "Table: sequences - 2489980 rows\n",
      "Table: sra_metadata - 100 rows\n",
      "\n",
      "=== Schema for each table ===\n",
      "\n",
      "Schema for annotations:\n",
      "  seqhash_id (VARCHAR) PRIMARY KEY\n",
      "  seed_ortholog (VARCHAR)\n",
      "  evalue (DOUBLE)\n",
      "  score (DOUBLE)\n",
      "  eggnog_ogs (VARCHAR)\n",
      "  max_annot_lvl (VARCHAR)\n",
      "  cog_category (VARCHAR)\n",
      "  description (VARCHAR)\n",
      "  preferred_name (VARCHAR)\n",
      "  sample_id (VARCHAR)\n",
      "\n",
      "Schema for clusters:\n",
      "  cluster_id (VARCHAR) PRIMARY KEY\n",
      "  representative_seqhash_id (VARCHAR)\n",
      "  size (INTEGER)\n",
      "\n",
      "Schema for cluster_members:\n",
      "  seqhash_id (VARCHAR) PRIMARY KEY\n",
      "  cluster_id (VARCHAR)\n",
      "\n",
      "Schema for ec_numbers:\n",
      "  seqhash_id (VARCHAR) PRIMARY KEY\n",
      "  ec_number (VARCHAR) PRIMARY KEY\n",
      "\n",
      "Schema for expression:\n",
      "  gene_seqhash_id (VARCHAR) PRIMARY KEY\n",
      "  sample_id (VARCHAR) PRIMARY KEY\n",
      "  tpm (DOUBLE)\n",
      "  num_reads (DOUBLE)\n",
      "  effective_length (DOUBLE)\n",
      "\n",
      "Schema for gene_protein_map:\n",
      "  gene_seqhash_id (VARCHAR) PRIMARY KEY\n",
      "  protein_seqhash_id (VARCHAR)\n",
      "\n",
      "Schema for go_terms:\n",
      "  seqhash_id (VARCHAR) PRIMARY KEY\n",
      "  go_term (VARCHAR) PRIMARY KEY\n",
      "\n",
      "Schema for kegg_info:\n",
      "  seqhash_id (VARCHAR) PRIMARY KEY\n",
      "  kegg_ko (VARCHAR)\n",
      "  kegg_pathway (VARCHAR)\n",
      "  kegg_module (VARCHAR)\n",
      "  kegg_reaction (VARCHAR)\n",
      "  kegg_rclass (VARCHAR)\n",
      "\n",
      "Schema for schema_version:\n",
      "  version (INTEGER) PRIMARY KEY\n",
      "  migration_name (VARCHAR)\n",
      "  applied_at (TIMESTAMP)\n",
      "\n",
      "Schema for sequences:\n",
      "  seqhash_id (VARCHAR) PRIMARY KEY\n",
      "  sequence (VARCHAR)\n",
      "  sample_id (VARCHAR)\n",
      "  assembly_date (TIMESTAMP)\n",
      "  is_representative (BOOLEAN)\n",
      "  repseq_id (VARCHAR)\n",
      "  length (INTEGER)\n",
      "\n",
      "Schema for sra_metadata:\n",
      "  sample_id (VARCHAR) PRIMARY KEY\n",
      "  organism (VARCHAR)\n",
      "  study_title (VARCHAR)\n",
      "  study_abstract (VARCHAR)\n",
      "  bioproject (VARCHAR)\n",
      "  biosample (VARCHAR)\n",
      "  library_strategy (VARCHAR)\n",
      "  library_source (VARCHAR)\n",
      "  library_selection (VARCHAR)\n",
      "  library_layout (VARCHAR)\n",
      "  instrument (VARCHAR)\n",
      "  run_spots (VARCHAR)\n",
      "  run_bases (VARCHAR)\n",
      "  run_published (VARCHAR)\n",
      "\n",
      "=== Gene-Protein Relationships ===\n",
      "Total genes: 745417\n",
      "Total proteins: 745417\n",
      "Total gene-protein relationships: 745417\n",
      "\n",
      "=== Expression Data ===\n",
      "Genes with expression data: 3089378\n",
      "Samples with expression data: 100\n",
      "Average TPM: 23.76\n",
      "Genes with both expression and protein mappings: 745415\n",
      "Proteins linked to genes with expression: 745415\n",
      "\n",
      "=== Annotation Data ===\n",
      "Total annotated sequences: 2049722\n",
      "Samples with annotations: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting cluster update for database: /mnt/data4/recombia.planter/master.duckdb\n",
      "Using clustering data from: /mnt/data4/planter_outputs/tmp/newClusterDB.tsv\n",
      "Duplicate handling strategy: replace\n",
      "Creating backup at: /mnt/data4/recombia.planter/master.duckdb.backup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated proteins: 622261\n",
      "Genes of annotated proteins: 622261\n",
      "Genes with both annotation and expression: 622259\n",
      "\n",
      "=== Cluster Data ===\n",
      "Total clusters: 428497\n",
      "Average cluster size: 1.01\n",
      "Largest cluster size: 11\n",
      "Unique sequences in clusters: 433791\n",
      "Total cluster membership records: 433791\n",
      "\n",
      "Top 3 largest clusters:\n",
      "  Cluster v1_DLS_929072aaf5bbeda34f7d58ac3e0d9fedd11ad3818a914aeb4fd7bb3e0001eaaf.p1: 11 members\n",
      "  Cluster v1_DLS_b6cf04a60dfba05cfefdf7b30e5096b124df4862e8ca750b224ce458186859b2.p1: 11 members\n",
      "  Cluster v1_DLS_9d3fd01247bdb20571289a7f0b4330da0fbabceb59cfbb805bbba9dd26ea40dc.p1: 10 members\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m validate_duckdb_schema(outdir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaster.duckdb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 4. Add cluster info to the master duckdb\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mupdate_clusters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutdir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaster.duckdb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtsv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackup_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/planter/planter/database/utils/duckdb_utils.py:386\u001b[0m, in \u001b[0;36mupdate_clusters\u001b[0;34m(db_path, tsv_path, backup_first, log_path, handle_duplicates)\u001b[0m\n\u001b[1;32m    384\u001b[0m     backup_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.backup\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating backup at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackup_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 386\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackup_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m con \u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;28mstr\u001b[39m(db_path))\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/shutil.py:463\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/shutil.py:273\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _USE_CP_SENDFILE:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m         \u001b[43m_fastcopy_sendfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfsrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfdst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dst\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m _GiveupOnFastCopy:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/shutil.py:150\u001b[0m, in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m         sent \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# ...in oder to have a more informative exception.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m         err\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m fsrc\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "samples = ['SRR12068547', 'SRR12068548', 'SRR12068549', 'SRR12068550', 'SRR12068551', 'SRR12068552']\n",
    "outdir = Path('/mnt/data4/recombia.planter')\n",
    "cluster_path = '/mnt/data4/planter_outputs/tmp/newClusterDB.tsv'\n",
    "\n",
    "# 1. Create duckdbs for Mesoplasma samples\n",
    "for sample in samples:\n",
    "    duckdb_path = outdir / f'{sample}/{sample}.duckdb'\n",
    "    if duckdb_path.exists():\n",
    "        # remove the duckdb file\n",
    "        duckdb_path.unlink()\n",
    "    create_duckdb(\n",
    "        sample_id=sample,\n",
    "        outdir=outdir,\n",
    "        duckdb_out=outdir / f'{sample}/{sample}.duckdb'\n",
    "    )\n",
    "\n",
    "# 2. Merge all duckdbs into a master duckdb\n",
    "merge_duckdbs(\n",
    "    duckdb_paths=[outdir / f'{sample}/{sample}.duckdb' for sample in samples],\n",
    "    master_db_path=outdir / 'master.duckdb',\n",
    "    schema_sql_path=Path('../planter/database/schema/migrations/004_add_gene_protein_map.sql'),\n",
    "    upgrade_schema=True,\n",
    "    target_schema_version=None\n",
    ")\n",
    "\n",
    "# 3. Validate the master duckdb\n",
    "validate_duckdb_schema(outdir / 'master.duckdb')\n",
    "\n",
    "# 4. Add cluster info to the master duckdb\n",
    "update_clusters(\n",
    "    db_path=outdir / 'master.duckdb',\n",
    "    tsv_path=cluster_path,\n",
    "    backup_first=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_duckdbs(\n",
    "#     duckdb_paths=[outdir / f'{sample}/{sample}.duckdb' for sample in samples],\n",
    "#     master_db_path=outdir / 'master.duckdb.backup',\n",
    "#     schema_sql_path=Path('../planter/database/schema/migrations/004_add_gene_protein_map.sql'),\n",
    "#     upgrade_schema=True,\n",
    "#     target_schema_version=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting cluster update for database: /mnt/data4/recombia.planter/master.duckdb\n",
      "Starting cluster update for database: /mnt/data4/recombia.planter/master.duckdb\n",
      "Using clustering data from: /tmp/newClusterDB.tsv\n",
      "Using clustering data from: /tmp/newClusterDB.tsv\n",
      "Duplicate handling strategy: replace\n",
      "Duplicate handling strategy: replace\n",
      "Creating backup at: /mnt/data4/recombia.planter/master.duckdb.backup\n",
      "Creating backup at: /mnt/data4/recombia.planter/master.duckdb.backup\n",
      "Beginning database transaction\n",
      "Beginning database transaction\n",
      "Loading cluster data from TSV...\n",
      "Loading cluster data from TSV...\n",
      "Loaded 434601 entries from clustering TSV\n",
      "Loaded 434601 entries from clustering TSV\n",
      "Strategy is 'replace': Dropping existing cluster tables...\n",
      "Strategy is 'replace': Dropping existing cluster tables...\n",
      "Recreating cluster tables...\n",
      "Recreating cluster tables...\n",
      "Identifying missing sequences...\n",
      "Identifying missing sequences...\n",
      "Found 405 unique sequences missing from database\n",
      "Found 405 unique sequences missing from database\n",
      "Found 192 unique representatives missing from database\n",
      "Found 192 unique representatives missing from database\n",
      "Examples of missing sequences:\n",
      "Examples of missing sequences:\n",
      "  1. v1_DLS_eb4f867e609bf0202cc00216a9ae369d8df9fc0b7c461cefa67a58d925d0c63b.p1\n",
      "  1. v1_DLS_eb4f867e609bf0202cc00216a9ae369d8df9fc0b7c461cefa67a58d925d0c63b.p1\n",
      "  2. v1_DLS_ca25be6edcfee03bf27ef56245a54ed3c84c1edee72b2e1ebe37db8d12bf4316.p1\n",
      "  2. v1_DLS_ca25be6edcfee03bf27ef56245a54ed3c84c1edee72b2e1ebe37db8d12bf4316.p1\n",
      "  3. v1_DLS_cfe8b090d5e003a5104f097f5345aebc723ea35c0943bcb4fe392027d265868e.p1\n",
      "  3. v1_DLS_cfe8b090d5e003a5104f097f5345aebc723ea35c0943bcb4fe392027d265868e.p1\n",
      "  4. v1_DLS_0638a90f44f329fbcc20927885725f0e01b4dfc8567e8cff34d79ba498ba8c5a.p1\n",
      "  4. v1_DLS_0638a90f44f329fbcc20927885725f0e01b4dfc8567e8cff34d79ba498ba8c5a.p1\n",
      "  5. v1_DLS_9731a78601e2a4935adca145075dac54ac34639daf3d906b8cbec61d16c2748a.p2\n",
      "  5. v1_DLS_9731a78601e2a4935adca145075dac54ac34639daf3d906b8cbec61d16c2748a.p2\n",
      "  6. v1_DLS_a2083df710f895865c66b09d8f1c1de62d1e9dd448c937075a2e56db5094013d.p1\n",
      "  6. v1_DLS_a2083df710f895865c66b09d8f1c1de62d1e9dd448c937075a2e56db5094013d.p1\n",
      "  7. v1_DLS_2ea6f1226602eec9d1e0140237dcb19fca17342955e5d57a128d2a167982441e.p1\n",
      "  7. v1_DLS_2ea6f1226602eec9d1e0140237dcb19fca17342955e5d57a128d2a167982441e.p1\n",
      "  8. v1_DLS_0158ef0579c5df39e5a14edc88717c47ad03948019e9dbf7d4b9720196abbe12.p1\n",
      "  8. v1_DLS_0158ef0579c5df39e5a14edc88717c47ad03948019e9dbf7d4b9720196abbe12.p1\n",
      "  9. v1_DLS_3343117b638cf44e750eabd87d94ef2cea5cf92d26a0514d9aee5265667eb6c0.p1\n",
      "  9. v1_DLS_3343117b638cf44e750eabd87d94ef2cea5cf92d26a0514d9aee5265667eb6c0.p1\n",
      "  10. v1_DLS_aa2f3239ff2194f1a969c2a82539df56b30d51663f240ff46c0da3ba923f1111.p1\n",
      "  10. v1_DLS_aa2f3239ff2194f1a969c2a82539df56b30d51663f240ff46c0da3ba923f1111.p1\n",
      "Filtering to valid entries only...\n",
      "Filtering to valid entries only...\n",
      "Retained 433711/434601 entries after filtering\n",
      "Retained 433711/434601 entries after filtering\n",
      "Updating sequences with representative information...\n",
      "Updating sequences with representative information...\n",
      "Updated 2489980 sequences with representative info\n",
      "Updated 2489980 sequences with representative info\n",
      "Creating clusters...\n",
      "Creating clusters...\n",
      "Created 428176 clusters\n",
      "Created 428176 clusters\n",
      "Adding cluster memberships with careful duplicate handling...\n",
      "Adding cluster memberships with careful duplicate handling...\n",
      "Inserted 433711 cluster memberships\n",
      "Inserted 433711 cluster memberships\n",
      "Final cluster_members count: 433711\n",
      "Final cluster_members count: 433711\n",
      "Average cluster size: 1.01\n",
      "Average cluster size: 1.01\n",
      "Largest cluster size: 11\n",
      "Largest cluster size: 11\n",
      "Successfully updated cluster information\n",
      "Successfully updated cluster information\n",
      "Log file saved to: /mnt/data4/recombia.planter/cluster_update_20250312_000102.log\n",
      "Log file saved to: /mnt/data4/recombia.planter/cluster_update_20250312_000102.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/data4/recombia.planter/master.duckdb.backup'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdir = Path('/mnt/data4/recombia.planter')\n",
    "# cluster_path = '/mnt/data4/planter_outputs/tmp/newClusterDB.tsv'\n",
    "cluster_path = '/tmp/newClusterDB.tsv'\n",
    "\n",
    "update_clusters(\n",
    "    db_path=outdir / 'master.duckdb',\n",
    "    tsv_path=cluster_path,\n",
    "    backup_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>seqhash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>310973</th>\n",
       "      <td>v1_DLS_388eb9d599d8964707f5e87139888647832fde5...</td>\n",
       "      <td>v1_DLS_388eb9d599d8964707f5e87139888647832fde5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               cluster_id  \\\n",
       "310973  v1_DLS_388eb9d599d8964707f5e87139888647832fde5...   \n",
       "\n",
       "                                               seqhash_id  \n",
       "310973  v1_DLS_388eb9d599d8964707f5e87139888647832fde5...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = pd.read_csv(cluster_path, sep='\\t', header=None, names=['cluster_id', 'seqhash_id'])\n",
    "clusters[clusters['cluster_id'] == 'v1_DLS_388eb9d599d8964707f5e87139888647832fde5f2994928ba82de8076adf5010.p1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging seqhash ID: v1_DLS_a2fb01ed3b3a4cd07a6f0b5063937e2c85390e49ef85f32b295474a9c5c7722f.p1\n",
      "\n",
      "== Database State ==\n",
      "ID exists in sequences table: False\n",
      "ID exists as a cluster_id: False\n",
      "ID exists in cluster_members table: False\n",
      "\n",
      "== TSV File Analysis ==\n",
      "ID appears as a representative in TSV: 1 times\n",
      "ID appears as a sequence in TSV: 2 times\n",
      "This ID clusters with representative: v1_DLS_ffddc1838d79d4e7db61ea10a6b5eabe6699f0c588ee0033c19083b741b2141b.p1\n",
      "\n",
      "POTENTIAL CIRCULAR REFERENCE DETECTED!\n",
      "This ID is both a representative sequence and a member of another cluster.\n",
      "Circular reference chain: v1_DLS_ffddc1838d79d4e7db61ea10a6b5eabe6699f0c588ee0033c19083b741b2141b.p1 -> v1_DLS_a2fb01ed3b3a4cd07a6f0b5063937e2c85390e49ef85f32b295474a9c5c7722f.p1 -> v1_DLS_a2fb01ed3b3a4cd07a6f0b5063937e2c85390e49ef85f32b295474a9c5c7722f.p1\n",
      "\n",
      "== Insertion Simulation ==\n",
      "Simulating the operations that might cause the constraint violation...\n",
      "ID doesn't exist in clusters or cluster_members tables yet.\n",
      "ID would pass validation filters: False\n",
      "\n",
      "== Recommendation ==\n",
      "Based on the analysis, consider using 'handle_duplicates=\"replace\"' to completely\n",
      "refresh the cluster tables, which should avoid any constraint violations.\n"
     ]
    }
   ],
   "source": [
    "def debug_specific_seqhash(\n",
    "    db_path: Union[str, Path],\n",
    "    tsv_path: Union[str, Path],\n",
    "    problem_seqhash: str = \"v1_DLS_388eb9d599d8964707f5e87139888647832fde5f2994928ba82de8076adf5010.p1\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Debug a specific seqhash ID that's causing constraint violations.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the DuckDB database file\n",
    "        tsv_path: Path to the TSV file containing cluster information\n",
    "        problem_seqhash: The specific sequence hash ID to investigate\n",
    "    \"\"\"\n",
    "    db_path = str(db_path)\n",
    "    tsv_path = str(tsv_path)\n",
    "    \n",
    "    print(f\"Debugging seqhash ID: {problem_seqhash}\")\n",
    "    \n",
    "    con = duckdb.connect(db_path)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n== Database State ==\")\n",
    "        \n",
    "        # Check if the ID exists in the sequences table\n",
    "        exists_in_sequences = con.execute(\n",
    "            f\"SELECT COUNT(*) FROM sequences WHERE seqhash_id = '{problem_seqhash}'\"\n",
    "        ).fetchone()[0]\n",
    "        print(f\"ID exists in sequences table: {exists_in_sequences > 0}\")\n",
    "        \n",
    "        # Check if it exists as a cluster_id\n",
    "        try:\n",
    "            exists_as_cluster = con.execute(\n",
    "                f\"SELECT COUNT(*) FROM clusters WHERE cluster_id = '{problem_seqhash}'\"\n",
    "            ).fetchone()[0]\n",
    "            print(f\"ID exists as a cluster_id: {exists_as_cluster > 0}\")\n",
    "        except:\n",
    "            print(\"clusters table doesn't exist or couldn't be queried\")\n",
    "        \n",
    "        # Check if it exists in cluster_members\n",
    "        try:\n",
    "            exists_as_member = con.execute(\n",
    "                f\"SELECT COUNT(*) FROM cluster_members WHERE seqhash_id = '{problem_seqhash}'\"\n",
    "            ).fetchone()[0]\n",
    "            print(f\"ID exists in cluster_members table: {exists_as_member > 0}\")\n",
    "        except:\n",
    "            print(\"cluster_members table doesn't exist or couldn't be queried\")\n",
    "        \n",
    "        print(\"\\n== TSV File Analysis ==\")\n",
    "        \n",
    "        # Load the TSV temporarily to analyze it\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE TEMP TABLE clustering_analysis AS\n",
    "            SELECT \n",
    "                column0 AS representative_seqhash_id,\n",
    "                column1 AS seqhash_id\n",
    "            FROM read_csv_auto('{tsv_path}', sep='\\t', header=FALSE)\n",
    "        \"\"\")\n",
    "        \n",
    "        # Check occurrences of this ID in the TSV\n",
    "        as_rep = con.execute(\n",
    "            f\"SELECT COUNT(*) FROM clustering_analysis WHERE representative_seqhash_id = '{problem_seqhash}'\"\n",
    "        ).fetchone()[0]\n",
    "        as_seq = con.execute(\n",
    "            f\"SELECT COUNT(*) FROM clustering_analysis WHERE seqhash_id = '{problem_seqhash}'\"\n",
    "        ).fetchone()[0]\n",
    "        \n",
    "        print(f\"ID appears as a representative in TSV: {as_rep} times\")\n",
    "        print(f\"ID appears as a sequence in TSV: {as_seq} times\")\n",
    "        \n",
    "        # If it appears as a sequence, show which cluster it belongs to\n",
    "        if as_seq > 0:\n",
    "            rep_id = con.execute(\n",
    "                f\"SELECT representative_seqhash_id FROM clustering_analysis WHERE seqhash_id = '{problem_seqhash}' LIMIT 1\"\n",
    "            ).fetchone()[0]\n",
    "            print(f\"This ID clusters with representative: {rep_id}\")\n",
    "        \n",
    "        # Check for circular references\n",
    "        if as_rep > 0 and as_seq > 0:\n",
    "            print(\"\\nPOTENTIAL CIRCULAR REFERENCE DETECTED!\")\n",
    "            print(\"This ID is both a representative sequence and a member of another cluster.\")\n",
    "            \n",
    "            # Get details of the circular reference\n",
    "            circle_info = con.execute(f\"\"\"\n",
    "                SELECT c1.representative_seqhash_id as rep1, c1.seqhash_id as seq1, \n",
    "                       c2.representative_seqhash_id as rep2\n",
    "                FROM clustering_analysis c1\n",
    "                JOIN clustering_analysis c2 ON c1.seqhash_id = c2.representative_seqhash_id\n",
    "                WHERE c1.seqhash_id = '{problem_seqhash}'\n",
    "                LIMIT 1\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            if circle_info:\n",
    "                print(f\"Circular reference chain: {circle_info[0][0]} -> {circle_info[0][1]} -> {circle_info[0][2]}\")\n",
    "        \n",
    "        print(\"\\n== Insertion Simulation ==\")\n",
    "        \n",
    "        # Try to simulate the insertion that might fail\n",
    "        print(\"Simulating the operations that might cause the constraint violation...\")\n",
    "        \n",
    "        # First check which table the constraint might be violated in\n",
    "        try:\n",
    "            if exists_as_member > 0:\n",
    "                print(\"This ID already exists in cluster_members table, insertion would violate PRIMARY KEY constraint\")\n",
    "            elif exists_as_cluster > 0:\n",
    "                print(\"This ID already exists in clusters table, insertion would violate PRIMARY KEY constraint\")\n",
    "            else:\n",
    "                print(\"ID doesn't exist in clusters or cluster_members tables yet.\")\n",
    "                \n",
    "                # Check if it would be inserted based on the filtering criteria\n",
    "                would_be_inserted = con.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) FROM clustering_analysis ca\n",
    "                    WHERE (ca.representative_seqhash_id = '{problem_seqhash}' OR ca.seqhash_id = '{problem_seqhash}')\n",
    "                    AND EXISTS (SELECT 1 FROM sequences s WHERE s.seqhash_id = ca.representative_seqhash_id)\n",
    "                    AND EXISTS (SELECT 1 FROM sequences s WHERE s.seqhash_id = ca.seqhash_id)\n",
    "                \"\"\").fetchone()[0]\n",
    "                \n",
    "                print(f\"ID would pass validation filters: {would_be_inserted > 0}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during simulation: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during debugging: {str(e)}\")\n",
    "    finally:\n",
    "        try:\n",
    "            con.execute(\"DROP TABLE IF EXISTS clustering_analysis\")\n",
    "        except:\n",
    "            pass\n",
    "        con.close()\n",
    "    \n",
    "    print(\"\\n== Recommendation ==\")\n",
    "    print(\"Based on the analysis, consider using 'handle_duplicates=\\\"replace\\\"' to completely\")\n",
    "    print(\"refresh the cluster tables, which should avoid any constraint violations.\")\n",
    "\n",
    "outdir = Path('/mnt/data4/recombia.planter')\n",
    "cluster_path = '/mnt/data4/planter_outputs/tmp/newClusterDB.tsv'\n",
    "debug_specific_seqhash(\n",
    "    db_path=outdir / 'master.duckdb',\n",
    "    tsv_path=cluster_path,\n",
    "    problem_seqhash=\"v1_DLS_a2fb01ed3b3a4cd07a6f0b5063937e2c85390e49ef85f32b295474a9c5c7722f.p1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotations': {'row_count': 2049722,\n",
       "  'columns': ['seqhash_id',\n",
       "   'seed_ortholog',\n",
       "   'evalue',\n",
       "   'score',\n",
       "   'eggnog_ogs',\n",
       "   'max_annot_lvl',\n",
       "   'cog_category',\n",
       "   'description',\n",
       "   'preferred_name',\n",
       "   'sample_id']},\n",
       " 'clusters': {'row_count': 433785,\n",
       "  'columns': ['cluster_id', 'representative_seqhash_id', 'size']},\n",
       " 'cluster_members': {'row_count': 2489980,\n",
       "  'columns': ['seqhash_id', 'cluster_id']},\n",
       " 'ec_numbers': {'row_count': 571387, 'columns': ['seqhash_id', 'ec_number']},\n",
       " 'expression': {'row_count': 3091553,\n",
       "  'columns': ['gene_seqhash_id',\n",
       "   'sample_id',\n",
       "   'tpm',\n",
       "   'num_reads',\n",
       "   'effective_length']},\n",
       " 'gene_protein_map': {'row_count': 744572,\n",
       "  'columns': ['gene_seqhash_id', 'protein_seqhash_id']},\n",
       " 'go_terms': {'row_count': 74845016, 'columns': ['seqhash_id', 'go_term']},\n",
       " 'kegg_info': {'row_count': 0,\n",
       "  'columns': ['seqhash_id',\n",
       "   'kegg_ko',\n",
       "   'kegg_pathway',\n",
       "   'kegg_module',\n",
       "   'kegg_reaction',\n",
       "   'kegg_rclass']},\n",
       " 'schema_version': {'row_count': 0,\n",
       "  'columns': ['version', 'migration_name', 'applied_at']},\n",
       " 'sequences': {'row_count': 2489980,\n",
       "  'columns': ['seqhash_id',\n",
       "   'sequence',\n",
       "   'sample_id',\n",
       "   'assembly_date',\n",
       "   'is_representative',\n",
       "   'repseq_id',\n",
       "   'length']},\n",
       " 'sra_metadata': {'row_count': 100,\n",
       "  'columns': ['sample_id',\n",
       "   'organism',\n",
       "   'study_title',\n",
       "   'study_abstract',\n",
       "   'bioproject',\n",
       "   'biosample',\n",
       "   'library_strategy',\n",
       "   'library_source',\n",
       "   'library_selection',\n",
       "   'library_layout',\n",
       "   'instrument',\n",
       "   'run_spots',\n",
       "   'run_bases',\n",
       "   'run_published']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_tables_in_db(db_path):\n",
    "    \"\"\"\n",
    "    Check which tables exist in a DuckDB database.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the DuckDB file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of table names and their row counts\n",
    "    \"\"\"\n",
    "    db_path = Path(db_path)\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        print(f\"Database file does not exist: {db_path}\")\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        with duckdb.connect(str(db_path)) as conn:\n",
    "            # Get all table names\n",
    "            tables = conn.execute(\n",
    "                \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "            ).fetchall()\n",
    "            \n",
    "            # Create a dictionary to store table info\n",
    "            table_info = {}\n",
    "            \n",
    "            # For each table, get row count and column info\n",
    "            for table_row in tables:\n",
    "                table_name = table_row[0]\n",
    "                \n",
    "                # Get row count\n",
    "                try:\n",
    "                    row_count = conn.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "                except Exception as e:\n",
    "                    row_count = f\"Error: {str(e)}\"\n",
    "                \n",
    "                # Get column info\n",
    "                try:\n",
    "                    columns = conn.execute(f\"PRAGMA table_info({table_name})\").fetchall()\n",
    "                    column_names = [col[1] for col in columns]\n",
    "                except Exception as e:\n",
    "                    column_names = [f\"Error: {str(e)}\"]\n",
    "                \n",
    "                # Store table info\n",
    "                table_info[table_name] = {\n",
    "                    \"row_count\": row_count,\n",
    "                    \"columns\": column_names\n",
    "                }\n",
    "            \n",
    "            return table_info\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to database: {str(e)}\")\n",
    "        return {}\n",
    "    \n",
    "# db_path = \"/mnt/data4/recombia.planter/SRR12068547/SRR12068547.duckdb\"\n",
    "db_path = '/mnt/data4/recombia.planter/master.duckdb'\n",
    "check_tables_in_db(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data4/recombia.planter/master.duckdb'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_path = '/mnt/data4/planter_outputs/tmp/newClusterDB.tsv'\n",
    "from planter.database.utils.duckdb_utils import (\n",
    "    update_clusters\n",
    ")\n",
    "\n",
    "update_clusters(\n",
    "    db_path=db_path,\n",
    "    tsv_path=cluster_path,\n",
    "    backup_first=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "clusters = pd.read_csv(cluster_path, sep='\\t', header=None, names=['cluster_id', 'seqhash_id'])\n",
    "clusters[clusters['cluster_id'] == 'v1_DLS_42f03501d692b370647a4bac7059434916aa1d21968fca917aa214f32f2ced17.p1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging foreign key constraints for ID: v1_DLS_19696ccb77b302cd0cdff9203ea61ab6436f11d2972fe64be97b00a5961373da.p1\n",
      "ID exists in clusters table: True\n",
      "Found 12 tables in the database\n",
      "Error checking foreign keys for table annotations: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "Error checking foreign keys for table cluster_members: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "Error checking foreign keys for table ec_numbers: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "Error checking foreign keys for table expression: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "Error checking foreign keys for table expression_backup: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "Error checking foreign keys for table gene_protein_map: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "Error checking foreign keys for table go_terms: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "Error checking foreign keys for table kegg_info: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "Error checking foreign keys for table schema_version: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "Error checking foreign keys for table sequences: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "Error checking foreign keys for table sra_metadata: Catalog Error: Pragma Function with name foreign_key_list does not exist!\n",
      "Did you mean \"force_checkpoint\"?\n",
      "No explicit references to the problematic ID were found.\n",
      "This might indicate a schema issue or a constraint that's not properly detected.\n",
      "\n",
      "Checking for ID in different formats:\n",
      "Table annotations.seqhash_id contains exact match: 1 rows\n",
      "Table clusters.cluster_id contains exact match: 1 rows\n",
      "Table clusters.representative_seqhash_id contains exact match: 1 rows\n",
      "Table cluster_members.seqhash_id contains exact match: 1 rows\n",
      "Table cluster_members.cluster_id contains exact match: 2 rows\n",
      "Table sequences.seqhash_id contains exact match: 1 rows\n",
      "Table sequences.repseq_id contains exact match: 2 rows\n",
      "\n",
      "Clusters table structure:\n",
      "  (0, 'cluster_id', 'VARCHAR', True, None, True)\n",
      "  (1, 'representative_seqhash_id', 'VARCHAR', True, None, False)\n",
      "  (2, 'size', 'INTEGER', True, None, False)\n",
      "\n",
      "Cluster_members table structure:\n",
      "  (0, 'seqhash_id', 'VARCHAR', True, None, True)\n",
      "  (1, 'cluster_id', 'VARCHAR', True, None, False)\n"
     ]
    }
   ],
   "source": [
    "def debug_foreign_key_issues(\n",
    "    db_path: Union[str, Path],\n",
    "    problematic_id: str = \"v1_DLS_19696ccb77b302cd0cdff9203ea61ab6436f11d2972fe64be97b00a5961373da.p1\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Debug foreign key constraint issues with a specific cluster ID.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the DuckDB database\n",
    "        problematic_id: The specific ID causing constraint issues\n",
    "    \"\"\"\n",
    "    db_path = str(db_path)\n",
    "    con = duckdb.connect(db_path)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Debugging foreign key constraints for ID: {problematic_id}\")\n",
    "        \n",
    "        # 1. First, check if this ID exists in the clusters table\n",
    "        cluster_exists = con.execute(\n",
    "            f\"SELECT COUNT(*) FROM clusters WHERE cluster_id = '{problematic_id}'\"\n",
    "        ).fetchone()[0]\n",
    "        print(f\"ID exists in clusters table: {cluster_exists > 0}\")\n",
    "        \n",
    "        # 2. Get all tables in the database\n",
    "        tables = con.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n",
    "        print(f\"Found {len(tables)} tables in the database\")\n",
    "        \n",
    "        # 3. Check each table to see if it references clusters\n",
    "        references = []\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            if table_name == 'clusters':\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Check if the table has foreign keys to clusters\n",
    "                fk_info = con.execute(f\"PRAGMA foreign_key_list({table_name})\").fetchall()\n",
    "                for fk in fk_info:\n",
    "                    if fk[2] == 'clusters':\n",
    "                        ref_table = table_name\n",
    "                        ref_col = fk[3]  # Referenced column in clusters\n",
    "                        local_col = fk[4]  # Local column in this table\n",
    "                        references.append((ref_table, local_col, ref_col))\n",
    "                        print(f\"Table {ref_table} references clusters.{ref_col} via {local_col}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking foreign keys for table {table_name}: {str(e)}\")\n",
    "        \n",
    "        # 4. For each table that references clusters, check if our problematic ID is referenced\n",
    "        found_references = False\n",
    "        for ref_table, local_col, ref_col in references:\n",
    "            try:\n",
    "                if ref_col == 'cluster_id':\n",
    "                    # Check if this table references our problematic ID\n",
    "                    ref_count = con.execute(\n",
    "                        f\"SELECT COUNT(*) FROM {ref_table} WHERE {local_col} = '{problematic_id}'\"\n",
    "                    ).fetchone()[0]\n",
    "                    \n",
    "                    if ref_count > 0:\n",
    "                        found_references = True\n",
    "                        print(f\"FOUND REFERENCE: Table {ref_table} has {ref_count} rows referencing the problematic ID\")\n",
    "                        \n",
    "                        # Get sample rows to understand the reference\n",
    "                        sample_rows = con.execute(\n",
    "                            f\"SELECT * FROM {ref_table} WHERE {local_col} = '{problematic_id}' LIMIT 3\"\n",
    "                        ).fetchall()\n",
    "                        print(f\"Sample rows from {ref_table}:\")\n",
    "                        for row in sample_rows:\n",
    "                            print(f\"  {row}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking references in table {ref_table}: {str(e)}\")\n",
    "        \n",
    "        if not found_references:\n",
    "            print(\"No explicit references to the problematic ID were found.\")\n",
    "            print(\"This might indicate a schema issue or a constraint that's not properly detected.\")\n",
    "            \n",
    "        # 5. Check if the ID might be in a different format or location\n",
    "        print(\"\\nChecking for ID in different formats:\")\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            try:\n",
    "                # Get column names for this table\n",
    "                columns = con.execute(f\"PRAGMA table_info({table_name})\").fetchall()\n",
    "                column_names = [col[1] for col in columns]\n",
    "                \n",
    "                # Check each column that might contain IDs\n",
    "                for col in column_names:\n",
    "                    if \"id\" in col.lower() or \"hash\" in col.lower() or \"key\" in col.lower():\n",
    "                        # Look for exact match\n",
    "                        exact_match = con.execute(\n",
    "                            f\"SELECT COUNT(*) FROM {table_name} WHERE {col} = '{problematic_id}'\"\n",
    "                        ).fetchone()[0]\n",
    "                        \n",
    "                        # Look for partial match\n",
    "                        partial_match = con.execute(\n",
    "                            f\"SELECT COUNT(*) FROM {table_name} WHERE {col} LIKE '%{problematic_id}%'\"\n",
    "                        ).fetchone()[0]\n",
    "                        \n",
    "                        if exact_match > 0:\n",
    "                            print(f\"Table {table_name}.{col} contains exact match: {exact_match} rows\")\n",
    "                        elif partial_match > 0:\n",
    "                            print(f\"Table {table_name}.{col} contains partial match: {partial_match} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking table {table_name} columns: {str(e)}\")\n",
    "                \n",
    "        # 6. Check if the clusters table has the expected structure\n",
    "        print(\"\\nClusters table structure:\")\n",
    "        cluster_cols = con.execute(\"PRAGMA table_info(clusters)\").fetchall()\n",
    "        for col in cluster_cols:\n",
    "            print(f\"  {col}\")\n",
    "            \n",
    "        print(\"\\nCluster_members table structure:\")\n",
    "        member_cols = con.execute(\"PRAGMA table_info(cluster_members)\").fetchall()\n",
    "        for col in member_cols:\n",
    "            print(f\"  {col}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during debugging: {str(e)}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "debug_foreign_key_issues(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for sequence ID: v1_DLS_a0a80e8508b66a50baba10818dc3b384cc13f0efb454dde8fc2c0dba1c936b19.p1\n",
      "ID exists in sequences table: False\n",
      "ID appears in TSV as sequence: True (1 times)\n",
      "ID appears in TSV as representative: False (0 times)\n",
      "Sample entries from TSV where ID is sequence:\n",
      "  ('v1_DLS_76400afa768372fd4118749f12b4252d972ea5024d4a582eaae1406cdf2c409b.p1', 'v1_DLS_a0a80e8508b66a50baba10818dc3b384cc13f0efb454dde8fc2c0dba1c936b19.p1')\n",
      "Total sequences in TSV missing from sequences table: 2585\n",
      "Sample of missing sequences:\n",
      "  1. v1_DLS_0aa8c2e83469e1ffc35fcd62036e912dc065345e35af76618ad10d60c93254a3.p1\n",
      "  2. v1_DLS_535ef37f0da2d881934117e345d4b080562c095bf4261380f2e186568bf6ce93.p1\n",
      "  3. v1_DLS_ad38a37bc806cbc7f948fa82d627b090ebd1dd34198b0690c51207a76b2c64ed.p1\n",
      "  4. v1_DLS_5e3b94322da686cedfffbf7ad8b27321fb217fb3036f886eb871e70d16898a1a.p1\n",
      "  5. v1_DLS_f973093a0658b4dddb4aefcece0e9bf8368a87a5b3708d8ee2a50330bdb478fb.p2\n",
      "  6. v1_DLS_b4948245d0398f054a1a41528136e3577c125a65504f2009ad6ea667c89ee57c.p1\n",
      "  7. v1_DLS_fcc1c5603a02c6e69f955dec7163f9189c23bf3c5f625d21a2e25e9a519ef284.p1\n",
      "  8. v1_DLS_7ee07f2ee58837aaa9b8d905a6de4a457448e1985247d2299c5c879e608b185b.p1\n",
      "  9. v1_DLS_53a16f034ddb2f317537fca07c0f9d1259bc42b3833f75f9b18724c8916eef8c.p1\n",
      "  10. v1_DLS_0638a90f44f329fbcc20927885725f0e01b4dfc8567e8cff34d79ba498ba8c5a.p1\n"
     ]
    }
   ],
   "source": [
    "def check_sequence_id(\n",
    "    db_path: Union[str, Path],\n",
    "    tsv_path: Union[str, Path],\n",
    "    seqhash_id: str = \"v1_DLS_a0a80e8508b66a50baba10818dc3b384cc13f0efb454dde8fc2c0dba1c936b19.p1\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Check if a specific sequence ID exists in the database and clustering TSV.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the DuckDB database\n",
    "        tsv_path: Path to the TSV file with cluster info\n",
    "        seqhash_id: The specific sequence ID to check\n",
    "    \"\"\"\n",
    "    db_path = str(db_path)\n",
    "    tsv_path = str(tsv_path)\n",
    "    \n",
    "    print(f\"Checking for sequence ID: {seqhash_id}\")\n",
    "    \n",
    "    # Check database tables\n",
    "    con = duckdb.connect(db_path)\n",
    "    try:\n",
    "        # Check if this ID exists in sequences table\n",
    "        exists_in_sequences = con.execute(\n",
    "            f\"SELECT COUNT(*) FROM sequences WHERE seqhash_id = '{seqhash_id}'\"\n",
    "        ).fetchone()[0]\n",
    "        \n",
    "        print(f\"ID exists in sequences table: {exists_in_sequences > 0}\")\n",
    "        \n",
    "        if exists_in_sequences > 0:\n",
    "            # Get details about this sequence\n",
    "            seq_details = con.execute(\n",
    "                f\"SELECT * FROM sequences WHERE seqhash_id = '{seqhash_id}'\"\n",
    "            ).fetchone()\n",
    "            print(f\"Sequence details: {seq_details}\")\n",
    "        \n",
    "        # Check TSV file for this ID\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE TEMP TABLE clustering_check AS\n",
    "            SELECT \n",
    "                column0 AS representative_seqhash_id,\n",
    "                column1 AS seqhash_id\n",
    "            FROM read_csv_auto('{tsv_path}', sep='\\t', header=FALSE)\n",
    "        \"\"\")\n",
    "        \n",
    "        # Check if ID appears as a sequence in clusters\n",
    "        as_sequence = con.execute(\n",
    "            f\"SELECT COUNT(*) FROM clustering_check WHERE seqhash_id = '{seqhash_id}'\"\n",
    "        ).fetchone()[0]\n",
    "        \n",
    "        # Check if ID appears as a representative\n",
    "        as_representative = con.execute(\n",
    "            f\"SELECT COUNT(*) FROM clustering_check WHERE representative_seqhash_id = '{seqhash_id}'\"\n",
    "        ).fetchone()[0]\n",
    "        \n",
    "        print(f\"ID appears in TSV as sequence: {as_sequence > 0} ({as_sequence} times)\")\n",
    "        print(f\"ID appears in TSV as representative: {as_representative > 0} ({as_representative} times)\")\n",
    "        \n",
    "        # Get sample of problematic entries from TSV\n",
    "        if as_sequence > 0:\n",
    "            sample = con.execute(\n",
    "                f\"SELECT * FROM clustering_check WHERE seqhash_id = '{seqhash_id}' LIMIT 3\"\n",
    "            ).fetchall()\n",
    "            print(f\"Sample entries from TSV where ID is sequence:\")\n",
    "            for entry in sample:\n",
    "                print(f\"  {entry}\")\n",
    "        \n",
    "        # Count total sequences in TSV not in sequences table\n",
    "        missing_seqs = con.execute(\"\"\"\n",
    "            SELECT COUNT(DISTINCT tc.seqhash_id) \n",
    "            FROM clustering_check tc\n",
    "            LEFT JOIN sequences s ON tc.seqhash_id = s.seqhash_id\n",
    "            WHERE s.seqhash_id IS NULL\n",
    "        \"\"\").fetchone()[0]\n",
    "        \n",
    "        print(f\"Total sequences in TSV missing from sequences table: {missing_seqs}\")\n",
    "        \n",
    "        # Get sample of missing sequences\n",
    "        if missing_seqs > 0:\n",
    "            sample_missing = con.execute(\"\"\"\n",
    "                SELECT DISTINCT tc.seqhash_id \n",
    "                FROM clustering_check tc\n",
    "                LEFT JOIN sequences s ON tc.seqhash_id = s.seqhash_id\n",
    "                WHERE s.seqhash_id IS NULL\n",
    "                LIMIT 10\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            print(\"Sample of missing sequences:\")\n",
    "            for i, (seq_id,) in enumerate(sample_missing, 1):\n",
    "                print(f\"  {i}. {seq_id}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during check: {str(e)}\")\n",
    "    finally:\n",
    "        try:\n",
    "            con.execute(\"DROP TABLE IF EXISTS clustering_check\")\n",
    "        except:\n",
    "            pass\n",
    "        con.close()\n",
    "\n",
    "check_sequence_id(db_path, cluster_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
